# [E] Production Cycle

1. Project scoping: A project starts with scoping the project, laying out goals & objectives, constraints, and evaluation criteria. Stakeholders should be identified and involved. Resources should be estimated and allocated.
2. Data management: Data used and generated by ML systems can be large and diverse, which requires scalable infrastructure to process and access it fast and reliably. Data management covers data sources, data formats, data processing, data control, data storage, etc.
3. ML model development: From raw data, you need to create training datasets and possibly label them, then generate features, train models, optimize models, and evaluate them. This is the stage that requires the most ML knowledge and is most often covered in ML courses.
4. Deployment: After a model is developed, it needs to be made accessible to users.
5. Monitoring and maintenance: Once in production, models need to be monitored for performance decay and maintained/updated to be adaptive to changing environments and changing requirements.
6. Business analysis: Model performance needs to be evaluated against business goals and analyzed to generate business insights. These insights can then be used to eliminate unproductive projects or scope out new projects.

# [E] Explain supervised, unsupervised, weakly supervised, semi-supervised, and active learning

- Supervised: all training data has a label assossicated with it.
- Unsupervised: training data has no label assossicated with it. Model tries to learn pattern. E.g. clustering based on training features.
- Weakly supervised: labels exist, but noisy, inexact, low resolution. E.g. image labeled "contains a cat" without bounding box.
- Semi-supervised: small labeled dataset + big unlabeled dataset. Uses unlabeled data to improve supervised learning. $L = L_{sup} + L_{unsup}$. $L_{unsup}$ can be: the model should give similar predictions for perturbed versions of the same input: $L_{unsup}=E_{x_u}\left[||f(x_u) - f(\text{augment}(x_u))||\right]$
- Active learning: Model chooses which sampels to label. Aims to minimize labeling cost while maximizing performance. Used to select the most informative examples to label. E.g. label points with highest prediction uncertainty, where models disagree.

# Empirical Risk Minimization

1. [E] What’s the risk in empirical risk minimization?
   1. Risk is the expected loss of the model on data drawn from true (unknown) distribution: $R(f) = E_{x,y \approx P}[L(f(x),y)]$
2. [E] Why is it empirical?
   1. Since we cannout compute expected value, because we don't know true distribution, we approximate it using mean: $R(f) = \sum^i_n\frac{1}{n}L(f(x_i),y_i)$
3. [E] How do we minimize that risk?
   1. We choose hypothesis $f$ from a model class that minimizes that risk. In practice, we define a loss function, e.g. MSE, compute loss, use optimization algorithm, e.g. SGD, to minimize the loss.

# [E] Occam's razor states that when the simple explanation and complex explanation both work equally well, the simple explanation is usually correct. How do we apply this principle in ML?

1. Use simpler model that contains fewer parameters and simpler architecture.
2. Use regularization to penalize complexity.
3. Instead of minimizing empirical risk alone, we choose a model class that balances training error + model complexity.

# [E] Why complex networks tend to overfit?

1. We are describing simple data with complex function. Remember quadratic function fitting linear data. Complex function also fits noise, it's learned as a signal.
2. The model memorizes instead of generalizing.
3. Bias-variance tradeoff: complex models have low bias, but high variance, i.e. small changes in data lead to large changes in the learned function.

# [E] What are the conditions that allowed deep learning to gain popularity in the last decade?

1. Data.
2. Compute power (access to GPU).
3. Frameworks.
4. Algorithm development.
5. Regularization.
6. Better benchmarks and evaluation (e.g. ImageNet).
7. Industry.

# [M] If we have a wide NN and a deep NN with the same number of parameters, which one is more expressive and why?

Deep NN. In wide NN, it learns just 1 function. In deep NN, it learns multiple functions stacked on top of each other. Composition of simpler functions can explain much more that just single simple function. For ReLU networks:

1. A depth-L network can represent functions with an exponential number of linear regions in L.
2. A shallow (wide) network needs exponentially more units to match this.
3. Thus, depth gives exponential gains in representational power per parameter.
4. First layers -> detailed features, future layers -> general features.
5. Theory: there exist functions that can be represented by deep NN with $O(n)$ parameters, but require $O(exp(n))$ parameters in wide NN.

# [H] The Universal Approximation Theorem states that a neural network with 1 hidden layer can approximate any continuous function for inputs within a specific range. Then why can’t a simple neural network reach an arbitrarily small positive error?

In theory it's true, but in practice it doesn't work like that.

1. The number of neurons required can be astronomically large.
2. It might take too much time to train such network.
3. UAT assumes optimal parameters exist, but in reality optimizer approximates without reaching it.
4. UAT assumes no noise, infinite data, exact target function.

# [E] What are saddle points and local minima? Which are thought to cause more problems for training large NNs?

1. Saddle points: a point where gradient is zero, but not a minimum nor maximum.
2. Local minima: a point where value is lower than of any other point in its neighborhood.
3. **IMPORTANT**:
   1. At a critical point, the Hessian has $d$ eigenvalues ($d$ = number of parameters).
   2. Local minimum: Hessian is positive definite, all eigenvalues $> 0$.
   3. Saddle point: Hessian has both positive and negative eigenvalues.
   4. As $d$ grows, the probability that all eigenvalues are positive goes to zero exponentially. Almost all critical points therefore become saddle points, not minima.
4. Thus:
   1. High-dimensional loss surfaces contain exponentially more saddle points than poor local minima.
   2. Most local minima in large overparameterized networks have similar, near-optimal loss.
   3. Gradient is almost zero on saddle points, which is bad for optimization algorithms.

# Hyperparameters

1. [E] What are the differences between parameters and hyperparameters?
   1. Parameters are learned by model (weights, biases), hyperparameters are set before training and control the learning process (e.g. lr, batch size).
2. [E] Why is hyperparameter tuning important?
   1. Control the training dynamics, model capacity, size, and therefore strongly affect convergence, performance, and generalization.
3. [M] Explain algorithm for tuning hyperparameters.
   1. We define a validation metric and search over hyperparameter values using methods such as grid search or random search. More advanced methods like Bayesian optimization or successive halving evaluate configurations iteratively and stop poorly performing ones early to save computation.

# [E] Zero-shot, One-shot, Few-shot Learning

Zero-shot, one-shot, and few-shot learning describe how a model performs a task using zero, one, or a few examples in the input prompt, without any additional training.

# [M] Attention

- Self-attention vs recurrent: Recurrent models cannot see future tokens because the math is recursive in time (h*t = f(h*{t−1}, x_t)); self-attention removes time recursion and replaces it with direct pairwise interactions between all tokens (output = AV = softmax( QKᵀ / sqrt(d) ), Q = X W_Q, K = X W_K, V = X W_V, X = [x_1, x_2, …, x_n])
- Self-attention: learns how each element in the sequence is connected with all other elements in the sequence by computing weighted combinations of their representations.
- Cross-attention: learns how elements of one sequence attend (connected) to elements of another sequence, typically used to connect an input sequence to an output sequence.
- Multi-head self-attention: runs multiple self-attention operations in parallel with different learned projections, allowing the model to capture different types of relationships (e.g. syntactic, semantic, positional) at the same time.
